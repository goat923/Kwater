{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b683b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.signal import butter, filtfilt\n",
    "import pywt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import shap\n",
    "import os\n",
    "import glob\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06954d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leakage_variable(data, structure):\n",
    "    if structure == 'A':\n",
    "        return data['Q1'] + data['Q2'] + data['Q3'] + data['Q4'] - data['Q5']\n",
    "    elif structure == 'B':\n",
    "        return data['Q1'] - (data['Q2'] + data['Q3'] + data['Q4'])\n",
    "    elif structure == 'C':\n",
    "        return data['Q1'] - (data['Q2'] + data['Q3'] + data['Q4'] + data['Q5'] + data['Q6'] + data['Q7'] + data['Q8'])\n",
    "    elif structure == 'D':\n",
    "        return data['Q1'] - (data['Q2'] + data['Q3'] + data['Q4'] + data['Q5'])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b086a",
   "metadata": {},
   "source": [
    "df_A = pd.read_csv(\"TRAIN_A.csv\")\n",
    "\n",
    "print(df_A.columns)\n",
    "print(\"칼럼 수 : \",len(df_A.columns))\n",
    "print(\"차원 : \",df_A.shape)\n",
    "\n",
    "\n",
    "# 새로운 열 생성: Q5 - (Q1 + Q2 + Q3 + Q4)\n",
    "df_A['leakage'] = df_A['Q1'] + df_A['Q2'] + df_A['Q3'] + df_A['Q4'] - df_A['Q5']\n",
    "#df_A['leakage_diff'] = df_A['leakage'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9664b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectionStage1(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.03, random_state=42, n_estimators=100):\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.iso_forest = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Isolation Forest 모델 정의 및 학습\n",
    "        self.iso_forest = IsolationForest(\n",
    "            contamination=self.contamination,\n",
    "            random_state=self.random_state,\n",
    "            n_estimators=self.n_estimators\n",
    "        )\n",
    "        leakage_data = X['leakage'].values.reshape(-1, 1)\n",
    "        self.iso_forest.fit(leakage_data)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        \n",
    "        # 2. 양수 leakage만 필터링\n",
    "        positive_mask = df['leakage'] > 0\n",
    "        df_positive = df[positive_mask].copy()\n",
    "\n",
    "        # 3. Anomaly 예측\n",
    "        leakage_data = df['leakage'].values.reshape(-1, 1)\n",
    "        anomaly_pred = self.iso_forest.predict(leakage_data)\n",
    "        df['predicted_anomaly'] = np.where(anomaly_pred == -1, 1, 0)\n",
    "\n",
    "        # predicted_anomaly를 stage1_anomaly로 변경\n",
    "        df = df.rename(columns={'predicted_anomaly': 'stage1_anomaly'})\n",
    "\n",
    "        # df_positive에도 stage1_anomaly 열 추가\n",
    "        df_positive['stage1_anomaly'] = df.loc[df_positive.index, 'stage1_anomaly']\n",
    "\n",
    "        # 결과를 원본 데이터프레임에 추가\n",
    "        df['df_positive_index'] = df.index.isin(df_positive.index)\n",
    "\n",
    "        return df  # 이 줄이 추가되었습니다\n",
    "\n",
    "# 파이프라인 사용 예시\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('stage1', AnomalyDetectionStage1())\n",
    "])\n",
    "\n",
    "# 파이프라인 실행\n",
    "result = pipeline.fit_transform(df_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294722da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectionStage2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.1, random_state=42, n_estimators=100):\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        pressure_cols = [col for col in df.columns if col.startswith('P') and col != 'Pi_flag']\n",
    "        \n",
    "        # 전처리 적용\n",
    "        preprocessed_df = self.preprocessing(df, pressure_cols)\n",
    "        \n",
    "        # 2단계 모델 적용\n",
    "        stage2_results = self.detect_pressure_anomalies_stage2(preprocessed_df, pressure_cols)\n",
    "        \n",
    "        # 1단계와 2단계 결과 결합\n",
    "        final_results = pd.DataFrame(index=df.index)\n",
    "        final_results['stage1_anomaly'] = df['stage1_anomaly']\n",
    "        final_results['stage2_anomaly'] = stage2_results['stage2_anomaly']\n",
    "\n",
    "        # AND 연산으로 두 단계에서 모두 anomaly로 예측된 경우만 선택\n",
    "        final_results['final_anomaly'] = ((final_results['stage1_anomaly'] == 1) & \n",
    "                                         (final_results['stage2_anomaly'] == 1)).astype(int)\n",
    "        \n",
    "        # 결과를 원본 데이터프레임에 추가\n",
    "        df = pd.concat([df, final_results], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def preprocessing(self, df, pressure_cols, ema_span=20):\n",
    "        preprocessed_features = {}\n",
    "        \n",
    "        for col in pressure_cols:\n",
    "            # 1. EMA로 노이즈 제거\n",
    "            ema = df[col].ewm(span=ema_span, adjust=False).mean()\n",
    "            \n",
    "            # 2. 하락 구간 식별\n",
    "            gradient = ema.diff()\n",
    "            drop_mask = gradient < 0\n",
    "            \n",
    "            # 3. 하락 구간 특성 추출\n",
    "            # 하락 기간: 연속된 하락 구간의 길이\n",
    "            drop_duration = pd.Series(0, index=df.index)\n",
    "            duration_count = 0\n",
    "            for i, is_dropping in enumerate(drop_mask):\n",
    "                if is_dropping:\n",
    "                    duration_count += 1\n",
    "                else:\n",
    "                    duration_count = 0\n",
    "                drop_duration.iloc[i] = duration_count\n",
    "            \n",
    "            # 하락 폭: 시작점 대비 현재 하락량\n",
    "            drop_magnitude = pd.Series(0.0, index=df.index)\n",
    "            start_value = ema.iloc[0]\n",
    "            for i, is_dropping in enumerate(drop_mask):\n",
    "                if is_dropping:\n",
    "                    drop_magnitude.iloc[i] = start_value - ema.iloc[i]\n",
    "                else:\n",
    "                    start_value = ema.iloc[i]\n",
    "            \n",
    "            # 하락 속도: gradient의 크기\n",
    "            drop_speed = gradient.abs() * drop_mask\n",
    "            \n",
    "            # 결과 저장\n",
    "            preprocessed_features[f'{col}_ema'] = ema\n",
    "            preprocessed_features[f'{col}_drop_duration'] = drop_duration\n",
    "            preprocessed_features[f'{col}_drop_magnitude'] = drop_magnitude\n",
    "            preprocessed_features[f'{col}_drop_speed'] = drop_speed\n",
    "        \n",
    "        return pd.DataFrame(preprocessed_features, index=df.index)\n",
    "\n",
    "    def detect_pressure_anomalies_stage2(self, preprocessed_df, pressure_cols):\n",
    "        drop_features = []\n",
    "        for col in pressure_cols:\n",
    "            drop_features.extend([\n",
    "                f'{col}_drop_duration',\n",
    "                f'{col}_drop_magnitude',\n",
    "                f'{col}_drop_speed'\n",
    "            ])\n",
    "        \n",
    "        features_df = preprocessed_df[drop_features].fillna(0)\n",
    "        \n",
    "        iso = IsolationForest(\n",
    "            contamination=self.contamination,\n",
    "            random_state=self.random_state,\n",
    "            n_estimators=self.n_estimators\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(features_df)\n",
    "        \n",
    "        predictions = iso.fit_predict(X_scaled)\n",
    "        anomalies = np.where(predictions == -1, 1, 0)\n",
    "        \n",
    "        results = pd.DataFrame(index=preprocessed_df.index)\n",
    "        results['stage2_anomaly'] = anomalies\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d276d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIFlagDetectionStage3(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window_size=3, min_duration=25, max_duration=35):\n",
    "        self.window_size = window_size\n",
    "        self.min_duration = min_duration\n",
    "        self.max_duration = max_duration\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        pressure_cols = [col for col in df.columns if col.startswith('P') and col != 'Pi_flag']\n",
    "        pi_flags = self.detect_pi_flags_stage3(df, df['final_anomaly'], pressure_cols, self.window_size)\n",
    "        \n",
    "        # pi_flags만 반환\n",
    "        return pi_flags\n",
    "\n",
    "    def find_anomaly_periods(self, anomaly_series):\n",
    "        periods = []\n",
    "        current_period = []\n",
    "        \n",
    "        for idx, value in anomaly_series.items():\n",
    "            if value == 1:\n",
    "                current_period.append(idx)\n",
    "            elif current_period:\n",
    "                if self.min_duration <= len(current_period) <= self.max_duration:\n",
    "                    periods.append(current_period)\n",
    "                current_period = []\n",
    "        \n",
    "        if current_period and self.min_duration <= len(current_period) <= self.max_duration:\n",
    "            periods.append(current_period)\n",
    "        \n",
    "        return periods\n",
    "\n",
    "    def detect_pi_flags_stage3(self, df_A, final_anomaly, pressure_cols, window_size=3):\n",
    "        # 전체 기간에 대한 Pi별 독립적 Isolation Forest 적용\n",
    "        flags = pd.DataFrame(0, index=df_A.index, columns=pressure_cols)\n",
    "        \n",
    "        # 1. 각 Pi별 독립적 anomaly 탐지\n",
    "        for i in range(len(pressure_cols) - window_size + 1):\n",
    "            current_group = pressure_cols[i:i+window_size]\n",
    "            \n",
    "            # 각 그룹별 하락 패턴 감지\n",
    "            for col in current_group:\n",
    "                iso = IsolationForest(contamination=0.1, random_state=42)\n",
    "                scaler = StandardScaler()\n",
    "                \n",
    "                # 압력 데이터의 하락 특성 추출\n",
    "                pressure_data = df_A[col].values.reshape(-1, 1)\n",
    "                gradient = np.gradient(pressure_data.flatten())\n",
    "                \n",
    "                features = np.column_stack([\n",
    "                    pressure_data,\n",
    "                    gradient,\n",
    "                    np.roll(gradient, 1)\n",
    "                ])\n",
    "                \n",
    "                X_scaled = scaler.fit_transform(features)\n",
    "                predictions = iso.fit_predict(X_scaled)\n",
    "                flags[col] = np.where(predictions == -1, 1, 0)\n",
    "        \n",
    "        # 2. Final anomaly 기간 동안의 연속된 압력계 하락 패턴 확인\n",
    "        final_flags = pd.DataFrame(0, index=df_A.index, columns=pressure_cols)\n",
    "        anomaly_periods = self.find_anomaly_periods(final_anomaly)\n",
    "        \n",
    "        for period in anomaly_periods:\n",
    "            # 각 period에서 가장 강한 하락 패턴을 보이는 연속된 압력계 그룹 찾기\n",
    "            max_drops = 0\n",
    "            best_group = None\n",
    "            \n",
    "            for i in range(len(pressure_cols) - window_size + 1):\n",
    "                current_group = pressure_cols[i:i+window_size]\n",
    "                group_flags = flags.loc[period, current_group]\n",
    "                \n",
    "                # 그룹의 하락 강도 계산\n",
    "                drop_strength = group_flags.sum().sum()\n",
    "                \n",
    "                if drop_strength > max_drops:\n",
    "                    max_drops = drop_strength\n",
    "                    best_group = current_group\n",
    "            \n",
    "            # 선택된 그룹에 대해서만 flag 설정\n",
    "            if best_group is not None:\n",
    "                final_flags.loc[period, best_group] = 1\n",
    "        \n",
    "        return final_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 사용 예시\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('stage1', AnomalyDetectionStage1()),\n",
    "    ('stage2', AnomalyDetectionStage2()),\n",
    "    ('stage3', PIFlagDetectionStage3())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c66a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 실행\n",
    "result = pipeline.fit_transform(df_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(df_A, result):\n",
    "    # 결과를 저장할 DataFrame 생성\n",
    "    submission = pd.DataFrame(columns=['ID', 'flag_list'])\n",
    "    \n",
    "    # A 구조의 압력계 개수 (P1부터 P26까지)\n",
    "    pressure_cols = [f'P{i}' for i in range(1, 27)]\n",
    "    \n",
    "    # pi_flags에서 Pi_flag가 포함된 컬럼들에 대해서만 이상 여부 확인\n",
    "    flags = [1 if result[f'P{i}_flag'].any() else 0 for i in range(1, 27)]\n",
    "    \n",
    "    # 결과 추가\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': ['A'],\n",
    "        'flag_list': [flags]\n",
    "    })\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "submission = create_submission(df_A, result)\n",
    "\n",
    "# 결과 저장\n",
    "submission.to_csv('sample_submission.csv', index=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(file_path, structure):\n",
    "    # 1. 파일 읽기 및 전처리\n",
    "    df_A = pd.read_csv(file_path)\n",
    "    df_A['leakage'] = create_leakage_variable(df_A, structure)\n",
    "    \n",
    "    # 2. 앙상블 모델 단계별 적용\n",
    "    pipeline = Pipeline([\n",
    "        ('stage1', AnomalyDetectionStage1()),\n",
    "        ('stage2', AnomalyDetectionStage2()),\n",
    "        ('stage3', PIFlagDetectionStage3())\n",
    "    ])\n",
    "    \n",
    "    # 파이프라인 실행\n",
    "    pi_flags = pipeline.fit_transform(df_A)\n",
    "    \n",
    "    # 3. 파일명 추출\n",
    "    file_name = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # 4. 결과 생성\n",
    "    if structure == 'C':\n",
    "        pressure_cols = [f'P{i}' for i in range(1, 9)]\n",
    "    else:  # structure == 'D'\n",
    "        pressure_cols = [f'P{i}' for i in range(1, 6)]\n",
    "    \n",
    "    flags = [1 if pi_flags[f'P{i}'].any() else 0 for i in range(1, len(pressure_cols) + 1)]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'ID': [file_name],\n",
    "        'flag_list': [flags]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b194db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_submission():\n",
    "    directory_path1 = \"C:/Users/leonk/OneDrive/바탕 화면/open/test/C\"\n",
    "    directory_path2 = \"C:/Users/leonk/OneDrive/바탕 화면/open/test/D\"\n",
    "    directories = [directory_path1, directory_path2]\n",
    "    \n",
    "    all_submissions = []\n",
    "    \n",
    "    for i, directory in enumerate(directories):\n",
    "        structure = 'C' if i == 0 else 'D'\n",
    "        csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "        \n",
    "        for file in csv_files:\n",
    "            submission = process_single_file(file, structure)\n",
    "            all_submissions.append(submission)\n",
    "    \n",
    "    if all_submissions:\n",
    "        final_submission = pd.concat(all_submissions, ignore_index=True)\n",
    "        final_submission.to_csv('submission.csv', index=False)\n",
    "        print(\"Submission file 'submission.csv' has been created.\")\n",
    "        return final_submission\n",
    "    else:\n",
    "        print(\"No CSV files found in the directories.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "create_final_submission()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
